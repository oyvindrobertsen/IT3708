%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.3 (9/9/13)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside]{article}

\usepackage{lipsum} % Package to generate dummy text throughout this template


\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage[utf8]{inputenc}
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage{amsmath}
\usepackage{listings}

\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true
}


%\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[margin={1cm,2cm}]{geometry}
\setlength{\columnsep}{1cm}
\usepackage{multicol} % Used for the two-column layout of the document
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables
\usepackage{float} % Required for tables and figures in the multi-column environment - they need to be placed in specific locations with the [H] (e.g. \begin{table}[H])
\usepackage{hyperref} % For hyperlinks in the PDF
\usepackage{multirow}

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text
\usepackage{paralist} % Used for the compactitem environment which makes bullet points with less space between them

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text


\usepackage{graphicx}

\usepackage{tikz}


\newcommand{\rparen}{)}

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
%\renewcommand{\thesubsection}{\thesection\hspace{1mm}\alph{subsection}}
\titleformat{\section}[block]{\large\scshape\centering}{\thesection}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{IT3708 Sub-symbolic AI Methods $\bullet$ Project 5 $\bullet$ \date{\today}} % Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\title{\vspace{-15mm}\fontsize{18pt}{10pt}\selectfont\textbf{Reinforcement learning using Q-Learning}} % Article title

\author{
    \large
    \textsc{Ã˜yvind Robertsen} \\ % Your name
    \normalsize Norwegian University of Science \& Technology \\ % Your institution
    \normalsize \href{mailto:oyvinrob@stud.ntnu.no}{oyvinrob@stud.ntnu.no} % Your email address
    \vspace{-5mm}
}
\date{}

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Insert title

\thispagestyle{fancy} % All pages have headers and footers

%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\begin{abstract}

    \noindent This report describes a solution to Project 5 in the subject IT3708 at NTNU. 
    The purpose of this project is to use reinforcement learning in a changing environment to learn an optimal policy.
    Specifically, we will apply Q-Learning to the Flatland environment from Project 3.
\end{abstract}

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\begin{multicols}{2} % Two-column layout throughout the main article text

    \section{Implementation}

    \subsection{Overview}

    My implementation is encapsulated in two classes.
    \texttt{Flatland}, representing the environment, and \texttt{FlatlandQLearn}, representing the learning agent.
    Given a \texttt{Flatland}-instance and some necessary configuration parameters, the agent will run the scenario repeatedly, updating its knowledgebase after each action made.
    When the algorithm is finished, the results are visualized by the \texttt{FlatlandGUI}-class.

    The key component of the Q-Learning algorithm is its knowledgebase $Q: State \times action \rightarrow value$.
    In the Flatland-environment, states are tuples of the agents position and the food it's eaten so far.
    Each time the agent performs an action, it gets immediate feedback from the environment on the quality of that action in the form of a numerical value.
    This value is then used to update the agents perception of that actions quality in the state the agent was just in.
    
    \begin{gather*}
        Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha (r_{t+1} + \gamma \max_{a}Q(s_t, a) - Q(s_t, a_t))
    \end{gather*}

    The equation above describes how the reward/punishment awarded for doing action $a_t$ in state $s_t$ influences the agents knowledge of that situation.
    Here, $\alpha$ is the learning rate and $\gamma$ is a discount rate, controlling to which degree the best value the agent can achieve in state $s_{t+1}$ will effect the value of doing action $a_t$ in $s_t$.

    Since we want to encourage eating food / finding a short path and discourage eating poison, the Flatland environment rewards a positive numerical value for any move that results in eating food or finishing the scenario.
    Both eating poison and moves without any effect other than movement are awarded negative values.
    The exact values have been tuned throughout my simulations.
    
    \subsection{The Q datastructure and lookup}

    I've implemented the Q-mapping of states and actions to values as a two-dimensional lookup-table by nesting Python dictionaries.
    States are instances of the \texttt{State} class, for which I've overridden the \texttt{\_\_hash\_\_} and \texttt{\_\_eq\_\_} methods to allow for instances to be used as dictionary keys.
    Looking up a state returns a new dictionary mapping actions to values.
    Querying for $max_{a}Q(s, a)$ then consists simply of \texttt{max(q.get(s).values())}.

    On large Flatland-scenarios, the lookup-table approach is not very memory efficient.
    A possible solution to this would be to replace the table with an approximated function.
    That is outside of the scope of this project however.

    \subsection{Parameter values}

    Q-Learning allows for some tuning of parameters. Their values and some reasoning is provided below.

    \textit{Number of iterations} - How many times the learning agent is to run the scenario. 
    For the smaller boards 1000 iterations is enough. On larger boards however, as many as 20000 iterations may be needed for the algorithm to converge on an optimal action, value-function.

    \textit{Learning rate} - Determines to what extent new information will override the old.
    In an unchanging environment, a learning rate of $1$ is optimal.
    In Flatland however, the environment is changing.
    To understand why a learning rate of $1$ is problematic for us, we consider the case of moving from A to B, eating poison in B, moving back to A and then moving back into B.
    The second time we move from A to B, the knowledge the agent had of A $\rightarrow$ B being a bad move because of poison, will be overwritten completely.
    Using $1$ as learning rate also leads to problems when we involve backup schemes.
    An example using TD(X) with $X > 1$ would be moving from A to B, eating poison in B, moving further on to C, eating food there.
    In this case, the negative value associated with moving from A to B and eating poison would be completely overwritten if a learning rate of $1$ was used.
    In most of my simulations I used a learning rate of $0.5$.

    \textit{Discount rate} - Determines the importance of future reward.
    A low value will make the agent consider the reward/punishment it was given for getting to a state, while largely ignoring the potential of the state.
    With a higher value, the agent will search for sequences of actions that yield high reward over time.
    Since the agent often will have to perform more than one action to get from one food to the other, we want to encourage this.
    Accordingly, I've used a relatively high discount rate of $0.8$ in many simulations.
    

    \section{Action selection}

    At each timestep, the agent has to choose an action to perform and subsequently be rewarded / punished for.
    The \texttt{FlatlandQLearn}-agent chooses actions randomly at first, transitioning into to exploiting it's knowledge in later iterations.
    A random action is chosen with probability $p$, and the best one with probability $1 - p$, with $p$ decreasing throughout iterations.
    This approach is similar to Simulated Annealing or Boltzmann selection for parent selection in EAs.
    
    Having the agent explore by mostly performing random actions at first allows it to gather knowledge of a great variety of scenarios before exploiting that knowledge at a later stage.

    \section{Backup scheme}

    I chose to implement support for both backup schemes; TD(X) as well as TD($\lambda$).
    On the large boards, using TD(X) can increase the speed at which knowledge propagates throughout the knowledgebase.
    The efficiency of this strategy is highly dependent on the value of $X$.
    Low values will have a less noticeable effect on knowledge propagation, while high values will reinforce actions that don't necessarily deserve reinforcing.
    
    When experimenting with TD($\lambda$), I noticed that it at best gave the same results as TD(X), but drastically increases run time.
    I also experimented with applying TD($\lambda$) only when certain conditions where met, such as when the agent returns to it's starting position after having eaten all food.
    These experiments yielded no significant results.
    


\end{multicols}

%\bibliography{references}
%\bibliographystyle{plain}

\end{document}
